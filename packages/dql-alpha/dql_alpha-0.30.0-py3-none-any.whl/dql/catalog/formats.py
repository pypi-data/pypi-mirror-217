import json
import os
import tarfile
import tempfile
from abc import ABC, abstractmethod
from contextlib import AbstractContextManager
from datetime import datetime, timezone
from itertools import groupby, islice
from typing import (
    TYPE_CHECKING,
    Any,
    Dict,
    Generic,
    Iterable,
    Iterator,
    List,
    Optional,
    Type,
    TypeVar,
    Union,
)

import attrs
from sqlalchemy import Float, Integer, String
from sqlalchemy.schema import DropTable
from tqdm import tqdm

from dql.data_storage.schema import SignalsTable
from dql.node import DirType, get_path

if TYPE_CHECKING:
    from types import TracebackType

    from dql.listing import Listing

PROCESSING_BATCH_SIZE = 1000  # Batch size for inserting entries.

INSERT_ITEMS = "insert"  # Indexing format adds new objects.
UPDATE_ITEMS = "update"  # Indexing format extends objects with new properties.


class Operation(AbstractContextManager, ABC):
    """Operation to apply to data generated by processors."""

    # This is defined outside the IndexingFormat class to allow
    # for batching operations.
    @abstractmethod
    async def __call__(self, items: Iterable[Any]):
        pass


class InsertNodes(Operation):
    """Operation that inserts nodes."""

    def __init__(self, listing: "Listing"):
        self.listing = listing

    async def __call__(self, items):
        await self.listing.data_storage.insert_entries(items)

    def __exit__(
        self,
        exc_type: Optional[Type[BaseException]],
        exc_val: Optional[BaseException],
        exc_tb: Optional["TracebackType"],
    ):
        self.listing.data_storage.inserts_done()


T = TypeVar("T")


class IndexingFormat(ABC, Generic[T]):
    """
    Indexing formats allow additional transformations on indexed
    objects, such as listing contents of archives.
    """

    @abstractmethod
    def begin(self, listing: "Listing") -> Operation:
        """
        Preprare for processing the nodes.
        This function returns an object that acts as a callable
        on batches of produced items and doubles as a context manager
        to cleanup after processing.
        """

    @abstractmethod
    def filter(self, listing: "Listing", paths: List[str]) -> Iterator[T]:
        """Create a list of entries to process"""

    @abstractmethod
    def process(self, listing, entries):
        """Process an entry and return additional entries to store."""


@attrs.define
class ArchiveInfo:
    parent: str
    name: str
    id: int
    is_latest: bool
    partial_id: int

    @property
    def path(self):
        return get_path(self.parent, self.name)


ARCHIVE_INFO_FIELDS = [attr.name for attr in attrs.fields(ArchiveInfo)]


class TarFiles(IndexingFormat[ArchiveInfo]):
    """
    TarFiles indexes buckets containing uncompressed tar archives. The contents of
    the archives is indexed as well.
    """

    processing_message = "Indexing tarball files"

    def begin(self, listing: "Listing") -> Operation:
        return InsertNodes(listing)

    def filter(self, listing: "Listing", paths: List[str]) -> Iterator[ArchiveInfo]:
        for path in paths:
            for node in listing.expand_path(path):
                found = listing.find(
                    node,
                    ARCHIVE_INFO_FIELDS,
                    names=["*.tar"],
                )
                for row in found:
                    yield ArchiveInfo(*row)

    def process(self, listing: "Listing", entries):
        for entry in entries:
            yield from self.process_entry(listing, entry)

    def process_entry(
        self, listing: "Listing", parent: ArchiveInfo
    ) -> Iterator[Dict[str, Any]]:
        local_path = tempfile.gettempdir() + f"/dql_cache_{parent.id}"
        client = listing.client
        # Download tarball to local storage first.
        client.fs.get_file(client.get_full_path(parent.path), local_path)
        with tarfile.open(name=local_path, mode="r:") as tar:
            for info in tar:
                if info.isdir():
                    yield self.tardir_from_info(info, parent)
                elif info.isfile():
                    yield self.tarmember_from_info(info, parent)
        os.remove(local_path)
        listing.data_storage.update_type(parent.id, DirType.TAR_ARCHIVE)

    def tarmember_from_info(self, info, parent: ArchiveInfo) -> Dict[str, Any]:
        location = json.dumps(
            [
                {
                    "offset": info.offset_data,
                    "size": info.size,
                    "type": "tar",
                    "parent": parent.path,
                },
            ]
        )
        full_path = f"{parent.path}/{info.name}"
        parent_dir, name = full_path.rsplit("/", 1)
        return {
            "vtype": "tar",
            "dir_type": DirType.FILE,
            "parent_id": parent.id,
            "parent": parent_dir,
            "name": name,
            "checksum": "",
            "etag": "",
            "version": "",
            "is_latest": parent.is_latest,
            "last_modified": datetime.fromtimestamp(info.mtime, timezone.utc),
            "size": info.size,
            "owner_name": info.uname,
            "owner_id": str(info.uid),
            "location": location,
            "partial_id": parent.partial_id,
        }

    def tardir_from_info(self, info, parent: ArchiveInfo) -> Dict[str, Any]:
        full_path = f"{parent.path}/{info.name}".rstrip("/")
        parent_dir, name = full_path.rsplit("/", 1)
        return {
            "vtype": "tar",
            "dir_type": DirType.DIR,
            "parent_id": parent.id,
            "parent": parent_dir,
            "name": name,
            "checksum": "",
            "etag": "",
            "version": "",
            "is_latest": parent.is_latest,
            "last_modified": datetime.fromtimestamp(info.mtime, timezone.utc),
            "size": info.size,
            "owner_name": info.uname,
            "owner_id": str(info.uid),
            "partial_id": parent.partial_id,
        }


@attrs.define
class ObjectInfo:
    parent: str
    name: str
    id: int
    vtype: str
    location: str

    @property
    def path(self):
        return get_path(self.parent, self.name)


OBJECT_INFO_FIELDS = [attr.name for attr in attrs.fields(ObjectInfo)]


class LaionJSONPair(IndexingFormat[ObjectInfo]):
    """
    Load signals from .json files and attach them to objects with the same base name.
    """

    processing_message = "Loading json annotations"

    IGNORED_EXTS = [".json", ".txt"]  # File extensions not to attach loaded signals to.

    def begin(self, listing: "Listing") -> Operation:
        return self.InsertSignals(listing)

    def filter(self, listing: "Listing", paths: List[str]) -> Iterator[ObjectInfo]:
        for path in paths:
            for node in listing.expand_path(path):
                found = listing.find(
                    node,
                    OBJECT_INFO_FIELDS,
                    order_by=["parent", "name"],
                )
                for row in found:
                    yield ObjectInfo(*row)

    def process(self, listing: "Listing", entries):
        for _, group in groupby(entries, self._group):
            yield from self._process_group(listing, group)

    def _process_group(self, listing: "Listing", group: Iterable[ObjectInfo]) -> Any:
        # Create a map of extension to object info.
        nodes = {os.path.splitext(obj.name)[1]: obj for obj in group}
        json_obj = nodes.get(".json")
        if not json_obj:
            # No .json file in group. Ignore.
            return
        with listing.client.open_object(
            json_obj.path, json_obj.vtype, json_obj.location
        ) as f:
            data = json.load(f)
            if not isinstance(data, dict):
                # .json file contains something other than a json object. Ignore.
                return
            values = {
                col_name: data.get(col_name) or self._default_value(col_type)
                for (col_name, col_type) in laionJSONColumns
            }

        for ext, node in nodes.items():
            if ext in self.IGNORED_EXTS:
                continue
            v = values.copy()
            v["id"] = node.id
            yield v

    def _default_value(self, col_type):
        if isinstance(col_type, Float):
            return 0.0
        elif isinstance(col_type, Integer):
            return 0
        elif isinstance(col_type, String):
            return ""
        return ""

    def _group(self, entry):
        """
        Group entries by paths sans the extension.
        This way 'path/000.jpg' and 'path/000.json' will be grouped
        together.
        """
        return os.path.splitext(entry.path)[0]

    class InsertSignals(Operation):
        """Insert rows into a signals table."""

        def __init__(self, listing: "Listing"):
            self.listing = listing
            self.table: SignalsTable

        async def __call__(self, items):
            q = self.table.insert().values(items)
            self.listing.data_storage.execute(q)

        def __enter__(self) -> "Operation":
            self.table = self.listing.data_storage.create_signals_table(
                self.listing.client.uri, laionJSONColumns
            )
            return self

        def __exit__(
            self,
            exc_type: Optional[Type[BaseException]],
            exc_val: Optional[BaseException],
            exc_tb: Optional["TracebackType"],
        ):
            ds = self.listing.data_storage
            try:
                if exc_val is None:
                    ds.extend_index_with_signals(ds.nodes, self.table)
            finally:
                ds.execute(DropTable(self.table.table))


# Signal columns for storing laion json data.
laionJSONColumns = [
    ("punsafe", Float()),
    ("pwatermark", Float()),
    ("similarity", Float()),
    ("hash", Integer()),
    ("caption", String()),
    ("url", String()),
    ("key", String()),
    ("status", String()),
    ("error_message", String()),
    ("width", Integer()),
    ("height", Integer()),
    ("original_width", Integer()),
    ("original_height", Integer()),
    ("md5", String()),
]


async def apply_processors(
    listing: "Listing", path: str, processors: List[IndexingFormat]
):
    for processor in processors:
        msg = getattr(processor, "processing_message", None or "Processing")
        with processor.begin(listing) as op:
            listing = listing.clone()
            with tqdm(desc=msg, unit=" objects") as pbar:
                entries = processor.filter(listing, [path])
                results = processor.process(listing, entries)
                for batch in _batch(results, PROCESSING_BATCH_SIZE):
                    pbar.update(len(batch))
                    await op(batch)


def _batch(it, size):
    while batch := list(islice(it, size)):
        yield batch


indexer_formats: Dict[str, Union[List[IndexingFormat], IndexingFormat]] = {
    "tar-files": TarFiles(),
    "json-pair": LaionJSONPair(),
    "webdataset": [TarFiles(), LaionJSONPair()],
}
